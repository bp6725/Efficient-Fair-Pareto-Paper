{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FairPareto Results Analysis\n",
    "\n",
    "This notebook analyzes the experimental results from the FairPareto evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import utilities\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.plotting import plot_multiple_pareto_fronts, plot_baseline_comparison\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path('../results')\n",
    "metrics_dir = results_dir / 'metrics'\n",
    "\n",
    "# Load all available results\n",
    "all_results = {}\n",
    "\n",
    "for results_file in metrics_dir.glob('*_pareto.pkl'):\n",
    "    dataset_name = results_file.stem.replace('_pareto', '')\n",
    "    with open(results_file, 'rb') as f:\n",
    "        all_results[dataset_name] = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded results for {len(all_results)} datasets:\")\n",
    "for name in sorted(all_results.keys()):\n",
    "    print(f\"  - {name}: {len(all_results[name])} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []\n",
    "\n",
    "for dataset_name, pareto_front in all_results.items():\n",
    "    gammas = list(pareto_front.keys())\n",
    "    accuracies = list(pareto_front.values())\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Points': len(pareto_front),\n",
    "        'Min Gamma': f\"{min(gammas):.4f}\",\n",
    "        'Max Gamma': f\"{max(gammas):.4f}\",\n",
    "        'Min Acc': f\"{min(accuracies):.4f}\",\n",
    "        'Max Acc': f\"{max(accuracies):.4f}\",\n",
    "        'Acc Range': f\"{max(accuracies) - min(accuracies):.4f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Individual Pareto Fronts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first few datasets\n",
    "datasets_to_plot = list(all_results.keys())[:6]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, dataset_name in enumerate(datasets_to_plot):\n",
    "    ax = axes[idx]\n",
    "    pf = all_results[dataset_name]\n",
    "    \n",
    "    gammas = sorted(pf.keys())\n",
    "    accs = [pf[g] for g in gammas]\n",
    "    \n",
    "    ax.plot(gammas, accs, 'go-', linewidth=2, markersize=6)\n",
    "    ax.set_xlabel('Statistical Parity (Î³)')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(dataset_name, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Multiple Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select datasets to compare\n",
    "datasets_to_compare = list(all_results.keys())[:5]\n",
    "comparison_data = {name: all_results[name] for name in datasets_to_compare}\n",
    "\n",
    "fig = plot_multiple_pareto_fronts(\n",
    "    comparison_data,\n",
    "    title='Pareto Fronts Comparison',\n",
    "    figsize=(12, 7)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Trade-off Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute trade-off slope for each dataset\n",
    "tradeoff_data = []\n",
    "\n",
    "for dataset_name, pf in all_results.items():\n",
    "    gammas = sorted(pf.keys())\n",
    "    accs = [pf[g] for g in gammas]\n",
    "    \n",
    "    # Compute average slope\n",
    "    slopes = []\n",
    "    for i in range(len(gammas) - 1):\n",
    "        delta_acc = accs[i+1] - accs[i]\n",
    "        delta_gamma = gammas[i+1] - gammas[i]\n",
    "        if delta_gamma > 0:\n",
    "            slopes.append(abs(delta_acc / delta_gamma))\n",
    "    \n",
    "    avg_slope = np.mean(slopes) if slopes else 0\n",
    "    \n",
    "    tradeoff_data.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Avg Trade-off Slope': avg_slope,\n",
    "        'Max Accuracy': max(accs),\n",
    "        'Accuracy Drop': max(accs) - min(accs)\n",
    "    })\n",
    "\n",
    "tradeoff_df = pd.DataFrame(tradeoff_data)\n",
    "tradeoff_df = tradeoff_df.sort_values('Avg Trade-off Slope', ascending=False)\n",
    "tradeoff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline comparison results if available\n",
    "comparison_results = {}\n",
    "\n",
    "for comp_file in metrics_dir.glob('*_comparison.pkl'):\n",
    "    dataset_name = comp_file.stem.replace('_comparison', '')\n",
    "    with open(comp_file, 'rb') as f:\n",
    "        comparison_results[dataset_name] = pickle.load(f)\n",
    "\n",
    "if comparison_results:\n",
    "    print(f\"Found baseline comparisons for {len(comparison_results)} datasets:\")\n",
    "    for name in comparison_results.keys():\n",
    "        print(f\"  - {name}\")\n",
    "else:\n",
    "    print(\"No baseline comparison results found.\")\n",
    "    print(\"Run: python experiments/baseline_comparison.py --datasets ADULT COMPAS LSAC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Summary for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LaTeX table for paper\n",
    "latex_table = summary_df.to_latex(index=False, float_format=\"%.4f\")\n",
    "print(\"LaTeX Table:\")\n",
    "print(latex_table)\n",
    "\n",
    "# Save to file\n",
    "with open(results_dir / 'summary_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"\\nTable saved to results/summary_table.tex\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
